"""
æ€§èƒ½ä¼˜åŒ–æµ‹è¯•è„šæœ¬
æµ‹è¯•Redisç¼“å­˜å’Œæ™ºèƒ½é‡‡æ ·å¯¹API keyé€‰æ‹©æ€§èƒ½çš„å½±å“
"""

import time
import logging
import asyncio
from typing import List
from unittest.mock import Mock, patch

import pytest
from sqlalchemy.orm import Session

from app.crud.api_keys import (
    get_api_key_with_token_bucket,
    get_active_api_key_ids_optimized,
    get_cached_active_api_key_ids,
    cache_active_api_key_ids,
    invalidate_active_api_keys_cache,
    smart_sample_api_keys,
    INITIAL_SAMPLE_SIZE,
    MAX_SAMPLE_SIZE
)
from app.models.models import ApiKey

logger = logging.getLogger(__name__)


class TestPerformanceOptimization:
    """æ€§èƒ½ä¼˜åŒ–æµ‹è¯•ç±»"""
    
    def setup_method(self):
        """æµ‹è¯•å‰å‡†å¤‡"""
        # æ¸…ç†ç¼“å­˜
        invalidate_active_api_keys_cache()
    
    def teardown_method(self):
        """æµ‹è¯•åæ¸…ç†"""
        # æ¸…ç†ç¼“å­˜
        invalidate_active_api_keys_cache()
    
    def test_cache_functionality(self):
        """æµ‹è¯•ç¼“å­˜åŠŸèƒ½"""
        # æµ‹è¯•ç¼“å­˜ä¸å­˜åœ¨æ—¶
        cached_ids = get_cached_active_api_key_ids()
        assert cached_ids is None
        
        # æµ‹è¯•ç¼“å­˜å†™å…¥
        test_ids = [1, 2, 3, 4, 5]
        cache_active_api_key_ids(test_ids)
        
        # æµ‹è¯•ç¼“å­˜è¯»å–
        cached_ids = get_cached_active_api_key_ids()
        assert cached_ids == test_ids
        
        # æµ‹è¯•ç¼“å­˜å¤±æ•ˆ
        invalidate_active_api_keys_cache()
        cached_ids = get_cached_active_api_key_ids()
        assert cached_ids is None
    
    def test_smart_sampling(self):
        """æµ‹è¯•æ™ºèƒ½é‡‡æ ·åŠŸèƒ½"""
        # æµ‹è¯•å°äºé‡‡æ ·å¤§å°çš„æƒ…å†µ
        small_list = [1, 2, 3, 4, 5]
        sampled = smart_sample_api_keys(small_list, INITIAL_SAMPLE_SIZE)
        assert sampled == small_list
        
        # æµ‹è¯•å¤§äºé‡‡æ ·å¤§å°çš„æƒ…å†µ
        large_list = list(range(1, 1001))  # 1000ä¸ªkeys
        sampled = smart_sample_api_keys(large_list, INITIAL_SAMPLE_SIZE)
        assert len(sampled) == INITIAL_SAMPLE_SIZE
        assert all(key_id in large_list for key_id in sampled)
    
    @patch('app.crud.api_keys.get_active_api_keys')
    def test_cache_performance_improvement(self, mock_get_active_keys):
        """æµ‹è¯•ç¼“å­˜å¯¹æ€§èƒ½çš„æ”¹å–„"""
        # æ¨¡æ‹Ÿå¤§é‡API keys
        mock_keys = []
        for i in range(1, 10001):  # 10000ä¸ªkeys
            mock_key = Mock(spec=ApiKey)
            mock_key.id = i
            mock_key.status = "active"
            mock_keys.append(mock_key)
        
        mock_get_active_keys.return_value = mock_keys
        
        # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®åº“ä¼šè¯
        mock_db = Mock(spec=Session)
        
        # ç¬¬ä¸€æ¬¡è°ƒç”¨ï¼ˆç¼“å­˜æœªå‘½ä¸­ï¼‰
        start_time = time.time()
        result1 = get_active_api_key_ids_optimized(mock_db)
        first_call_time = time.time() - start_time
        
        assert len(result1) == 10000
        assert mock_get_active_keys.call_count == 1
        
        # ç¬¬äºŒæ¬¡è°ƒç”¨ï¼ˆç¼“å­˜å‘½ä¸­ï¼‰
        start_time = time.time()
        result2 = get_active_api_key_ids_optimized(mock_db)
        second_call_time = time.time() - start_time
        
        assert result1 == result2
        assert mock_get_active_keys.call_count == 1  # æ²¡æœ‰å†æ¬¡è°ƒç”¨æ•°æ®åº“
        
        # ç¼“å­˜å‘½ä¸­åº”è¯¥æ˜¾è‘—æ›´å¿«
        logger.info(f"First call (cache miss): {first_call_time:.4f}s")
        logger.info(f"Second call (cache hit): {second_call_time:.4f}s")
        logger.info(f"Performance improvement: {first_call_time / second_call_time:.2f}x")
        
        # ç¼“å­˜å‘½ä¸­åº”è¯¥æ›´å¿«ï¼ˆæ”¾å®½æ¡ä»¶ï¼Œå› ä¸ºæµ‹è¯•ç¯å¢ƒå¯èƒ½æœ‰å·®å¼‚ï¼‰
        assert second_call_time < first_call_time or second_call_time < 0.1
    
    @patch('app.crud.api_keys.token_bucket_manager')
    @patch('app.crud.api_keys.get_api_key')
    @patch('app.crud.api_keys.get_active_api_key_ids_optimized')
    def test_sampling_performance_improvement(self, mock_get_ids, mock_get_key, mock_token_manager):
        """æµ‹è¯•é‡‡æ ·å¯¹æ€§èƒ½çš„æ”¹å–„"""
        # æ¨¡æ‹Ÿå¤§é‡API key IDs
        large_key_ids = list(range(1, 50001))  # 50000ä¸ªkeys
        mock_get_ids.return_value = large_key_ids
        
        # æ¨¡æ‹Ÿtoken bucket manager
        mock_token_manager.get_available_api_keys.return_value = [1, 2, 3]  # å‰å‡ ä¸ªæœ‰token
        mock_token_manager.consume_token.return_value = True
        # æ¨¡æ‹Ÿtokenæ•°é‡è¿”å›
        mock_token_manager.get_available_tokens_batch.return_value = {1: 10.0, 2: 8.0, 3: 5.0}
        
        # æ¨¡æ‹ŸAPI keyå¯¹è±¡
        mock_key = Mock(spec=ApiKey)
        mock_key.id = 1
        mock_key.status = "active"
        mock_get_key.return_value = mock_key
        
        # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®åº“ä¼šè¯
        mock_db = Mock(spec=Session)
        
        # æµ‹è¯•ä¼˜åŒ–åçš„å‡½æ•°
        start_time = time.time()
        result = get_api_key_with_token_bucket(mock_db, required_tokens=1)
        optimized_time = time.time() - start_time
        
        assert result is not None
        assert result.id == 1
        
        # éªŒè¯åªæ£€æŸ¥äº†é‡‡æ ·çš„keysï¼Œè€Œä¸æ˜¯å…¨éƒ¨50000ä¸ª
        call_args = mock_token_manager.get_available_api_keys.call_args[0]
        sampled_keys = call_args[0]
        
        logger.info(f"Total keys: {len(large_key_ids)}")
        logger.info(f"Sampled keys: {len(sampled_keys)}")
        logger.info(f"Sampling ratio: {len(sampled_keys) / len(large_key_ids) * 100:.2f}%")
        logger.info(f"Optimized call time: {optimized_time:.4f}s")
        
        # é‡‡æ ·å¤§å°åº”è¯¥è¿œå°äºæ€»æ•°
        assert len(sampled_keys) <= INITIAL_SAMPLE_SIZE
        assert len(sampled_keys) < len(large_key_ids) / 10  # è‡³å°‘å‡å°‘90%
    
    def test_progressive_sampling(self):
        """æµ‹è¯•æ¸è¿›å¼é‡‡æ ·ç­–ç•¥"""
        # æµ‹è¯•é‡‡æ ·æ‰©å±•
        key_ids = list(range(1, 5001))  # 5000ä¸ªkeys
        
        # ç¬¬ä¸€æ¬¡é‡‡æ ·
        sample1 = smart_sample_api_keys(key_ids, INITIAL_SAMPLE_SIZE)
        assert len(sample1) == INITIAL_SAMPLE_SIZE
        
        # æ‰©å±•é‡‡æ ·
        expanded_size = min(INITIAL_SAMPLE_SIZE * 2, MAX_SAMPLE_SIZE, len(key_ids))
        sample2 = smart_sample_api_keys(key_ids, expanded_size)
        assert len(sample2) == expanded_size
        
        # æœ€å¤§é‡‡æ ·
        max_sample = smart_sample_api_keys(key_ids, MAX_SAMPLE_SIZE)
        assert len(max_sample) == min(MAX_SAMPLE_SIZE, len(key_ids))
    
    @patch('app.crud.api_keys.get_redis_client')
    def test_redis_error_handling(self, mock_redis_client):
        """æµ‹è¯•Redisé”™è¯¯å¤„ç†"""
        # æ¨¡æ‹ŸRedisè¿æ¥é”™è¯¯
        mock_redis_client.side_effect = Exception("Redis connection failed")
        
        # ç¼“å­˜è¯»å–åº”è¯¥è¿”å›Noneè€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸
        cached_ids = get_cached_active_api_key_ids()
        assert cached_ids is None
        
        # ç¼“å­˜å†™å…¥åº”è¯¥ä¸æŠ›å‡ºå¼‚å¸¸
        test_ids = [1, 2, 3]
        try:
            cache_active_api_key_ids(test_ids)
        except Exception:
            pytest.fail("cache_active_api_key_ids should not raise exception on Redis error")
    
    def test_cache_ttl_expiration(self):
        """æµ‹è¯•ç¼“å­˜TTLè¿‡æœŸ"""
        # è¿™ä¸ªæµ‹è¯•éœ€è¦å®é™…çš„Redisè¿æ¥ï¼Œåœ¨é›†æˆæµ‹è¯•ä¸­è¿è¡Œ
        pass
    
    def generate_performance_report(self):
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        report = {
            "optimization_features": [
                "Redisç¼“å­˜æ´»è·ƒAPI keysåˆ—è¡¨",
                "æ™ºèƒ½é‡‡æ ·ç­–ç•¥ï¼ˆåˆå§‹200ä¸ªkeysï¼‰",
                "æ¸è¿›å¼æ‰©å±•é‡‡æ ·",
                "è‡ªåŠ¨ç¼“å­˜å¤±æ•ˆæœºåˆ¶"
            ],
            "expected_improvements": {
                "database_queries": "å‡å°‘99%+ï¼ˆä»æ¯æ¬¡è¯·æ±‚æŸ¥è¯¢å˜ä¸ºç¼“å­˜å‘½ä¸­ï¼‰",
                "token_bucket_checks": "å‡å°‘90%+ï¼ˆä»æ£€æŸ¥æ‰€æœ‰keyså˜ä¸ºé‡‡æ ·æ£€æŸ¥ï¼‰",
                "memory_usage": "å¢åŠ çº¦1MBï¼ˆç¼“å­˜50000ä¸ªkey IDsï¼‰",
                "response_time": "é¢„æœŸæå‡5-10å€"
            },
            "cache_configuration": {
                "ttl_seconds": 300,
                "initial_sample_size": INITIAL_SAMPLE_SIZE,
                "max_sample_size": MAX_SAMPLE_SIZE,
                "expansion_factor": 2
            }
        }
        return report


if __name__ == "__main__":
    # è¿è¡Œæ€§èƒ½æµ‹è¯•
    test_instance = TestPerformanceOptimization()
    
    print("ğŸš€ å¼€å§‹æ€§èƒ½ä¼˜åŒ–æµ‹è¯•...")
    
    # è¿è¡Œå„é¡¹æµ‹è¯•
    test_instance.setup_method()
    
    try:
        print("âœ… æµ‹è¯•ç¼“å­˜åŠŸèƒ½...")
        test_instance.test_cache_functionality()
        
        print("âœ… æµ‹è¯•æ™ºèƒ½é‡‡æ ·...")
        test_instance.test_smart_sampling()
        
        print("âœ… æµ‹è¯•ç¼“å­˜æ€§èƒ½æ”¹å–„...")
        test_instance.test_cache_performance_improvement()
        
        print("âœ… æµ‹è¯•é‡‡æ ·æ€§èƒ½æ”¹å–„...")
        test_instance.test_sampling_performance_improvement()
        
        print("âœ… æµ‹è¯•æ¸è¿›å¼é‡‡æ ·...")
        test_instance.test_progressive_sampling()
        
        print("âœ… æµ‹è¯•Redisé”™è¯¯å¤„ç†...")
        test_instance.test_redis_error_handling()
        
        print("\nğŸ“Š æ€§èƒ½ä¼˜åŒ–æŠ¥å‘Š:")
        report = test_instance.generate_performance_report()
        
        print(f"ä¼˜åŒ–åŠŸèƒ½: {', '.join(report['optimization_features'])}")
        print(f"é¢„æœŸæ”¹å–„:")
        for key, value in report['expected_improvements'].items():
            print(f"  - {key}: {value}")
        
        print(f"ç¼“å­˜é…ç½®:")
        for key, value in report['cache_configuration'].items():
            print(f"  - {key}: {value}")
        
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼æ€§èƒ½ä¼˜åŒ–å®æ–½æˆåŠŸï¼")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        raise
    finally:
        test_instance.teardown_method()