import time
import json
import logging
from typing import Optional, List, Dict, Any
from dataclasses import dataclass, asdict
import redis
from ..core.config import settings

logger = logging.getLogger(__name__)


@dataclass
class TokenBucket:
    """ä»¤ç‰Œæ¡¶æ•°æ®ç»“æ„"""
    capacity: int  # æ¡¶å®¹é‡
    tokens: float  # å½“å‰ä»¤ç‰Œæ•°
    refill_rate: float  # æ¯ç§’è¡¥å……ä»¤ç‰Œæ•°
    last_refill: float  # ä¸Šæ¬¡è¡¥å……æ—¶é—´æˆ³
    
    def to_dict(self) -> Dict[str, Any]:
        return asdict(self)
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'TokenBucket':
        return cls(**data)


class TokenBucketManager:
    """ä»¤ç‰Œæ¡¶ç®¡ç†å™¨ï¼Œä½¿ç”¨ Redis å­˜å‚¨çŠ¶æ€"""
    
    def __init__(self, redis_client: Optional[redis.Redis] = None):
        if redis_client is None:
            self.redis_client = redis.from_url(
                settings.REDIS_URL,
                password=settings.REDIS_PASSWORD,
                decode_responses=True
            )
        else:
            self.redis_client = redis_client
        
        # é»˜è®¤é…ç½®
        self.default_capacity = 20  # é»˜è®¤æ¡¶å®¹é‡
        self.default_refill_rate = 1.0  # é»˜è®¤æ¯ç§’è¡¥å……1ä¸ªä»¤ç‰Œ
        self.bucket_prefix = "token_bucket:api_key:"
        self.bucket_ttl = 3600  # æ¡¶æ•°æ®TTLï¼ˆç§’ï¼‰
    
    def _get_bucket_key(self, api_key_id: int) -> str:
        """è·å– Redis ä¸­ä»¤ç‰Œæ¡¶çš„é”®å"""
        return f"{self.bucket_prefix}{api_key_id}"
    
    def _get_bucket(self, api_key_id: int) -> TokenBucket:
        """ä» Redis è·å–ä»¤ç‰Œæ¡¶ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»ºæ–°çš„"""
        bucket_key = self._get_bucket_key(api_key_id)
        bucket_data = self.redis_client.get(bucket_key)
        
        if bucket_data:
            try:
                data = json.loads(bucket_data)
                return TokenBucket.from_dict(data)
            except (json.JSONDecodeError, TypeError) as e:
                logger.warning(f"Failed to parse bucket data for key {api_key_id}: {e}")
        
        # åˆ›å»ºæ–°çš„ä»¤ç‰Œæ¡¶
        current_time = time.time()
        return TokenBucket(
            capacity=self.default_capacity,
            tokens=self.default_capacity,  # åˆå§‹æ—¶æ¡¶æ˜¯æ»¡çš„
            refill_rate=self.default_refill_rate,
            last_refill=current_time
        )
    
    def _save_bucket(self, api_key_id: int, bucket: TokenBucket):
        """å°†ä»¤ç‰Œæ¡¶ä¿å­˜åˆ° Redis"""
        bucket_key = self._get_bucket_key(api_key_id)
        bucket_data = json.dumps(bucket.to_dict())
        self.redis_client.setex(bucket_key, self.bucket_ttl, bucket_data)
    
    def _refill_bucket(self, bucket: TokenBucket) -> TokenBucket:
        """è¡¥å……ä»¤ç‰Œæ¡¶ä¸­çš„ä»¤ç‰Œ"""
        current_time = time.time()
        time_passed = current_time - bucket.last_refill
        
        if time_passed > 0:
            # è®¡ç®—åº”è¯¥è¡¥å……çš„ä»¤ç‰Œæ•°
            tokens_to_add = time_passed * bucket.refill_rate
            bucket.tokens = min(bucket.capacity, bucket.tokens + tokens_to_add)
            bucket.last_refill = current_time
        
        return bucket
    
    def _consume_token_lua_script(self):
        """è·å–Luaè„šæœ¬ç”¨äºåŸå­æ€§ä»¤ç‰Œæ¶ˆè€—"""
        return """
        local bucket_key = KEYS[1]
        local tokens_to_consume = tonumber(ARGV[1])
        local current_time = tonumber(ARGV[2])
        local capacity = tonumber(ARGV[3])
        local refill_rate = tonumber(ARGV[4])
        local ttl = tonumber(ARGV[5])
        
        local bucket_data = redis.call('GET', bucket_key)
        local bucket
        
        if bucket_data then
            bucket = cjson.decode(bucket_data)
        else
            bucket = {
                capacity = capacity,
                tokens = capacity,
                refill_rate = refill_rate,
                last_refill = current_time
            }
        end
        
        -- è¡¥å……ä»¤ç‰Œ
        local time_passed = current_time - bucket.last_refill
        if time_passed > 0 then
            local tokens_to_add = time_passed * bucket.refill_rate
            bucket.tokens = math.min(bucket.capacity, bucket.tokens + tokens_to_add)
            bucket.last_refill = current_time
        end
        
        -- å°è¯•æ¶ˆè€—ä»¤ç‰Œ
        local success = false
        if bucket.tokens >= tokens_to_consume then
            bucket.tokens = bucket.tokens - tokens_to_consume
            success = true
        end
        
        -- ä¿å­˜æ¡¶çŠ¶æ€
        redis.call('SETEX', bucket_key, ttl, cjson.encode(bucket))
        
        return {success and 1 or 0, bucket.tokens}
        """
    
    def consume_token(self, api_key_id: int, tokens: int = 1) -> bool:
        """
        å°è¯•ä»ä»¤ç‰Œæ¡¶ä¸­æ¶ˆè€—æŒ‡å®šæ•°é‡çš„ä»¤ç‰Œï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼Œä½¿ç”¨Luaè„šæœ¬ï¼‰
        
        Args:
            api_key_id: API key ID
            tokens: è¦æ¶ˆè€—çš„ä»¤ç‰Œæ•°é‡
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸæ¶ˆè€—ä»¤ç‰Œ
        """
        try:
            # å°è¯•ä½¿ç”¨Luaè„šæœ¬è¿›è¡ŒåŸå­æ“ä½œ
            if hasattr(self, '_lua_script'):
                logger.info(f"ğŸš€ [TOKEN BUCKET] Using optimized Lua script for API key {api_key_id}")
                bucket_key = self._get_bucket_key(api_key_id)
                current_time = time.time()
                
                result = self._lua_script(
                    keys=[bucket_key],
                    args=[tokens, current_time, self.default_capacity, self.default_refill_rate, self.bucket_ttl]
                )
                
                success = bool(result[0])
                remaining_tokens = float(result[1])
                
                if success:
                    logger.info(f"âœ… [TOKEN BUCKET] Successfully consumed {tokens} tokens for API key {api_key_id}, remaining: {remaining_tokens:.2f}")
                else:
                    logger.warning(f"âŒ [TOKEN BUCKET] Insufficient tokens for API key {api_key_id}, available: {remaining_tokens:.2f}, required: {tokens}")
                
                return success
            else:
                # å›é€€åˆ°åŸæœ‰å®ç°
                logger.info(f"âš ï¸ [TOKEN BUCKET] Using fallback implementation for API key {api_key_id}")
                bucket = self._get_bucket(api_key_id)
                bucket = self._refill_bucket(bucket)
                
                if bucket.tokens >= tokens:
                    bucket.tokens -= tokens
                    self._save_bucket(api_key_id, bucket)
                    logger.info(f"âœ… [TOKEN BUCKET] Consumed {tokens} tokens for API key {api_key_id}, remaining: {bucket.tokens:.2f}")
                    return True
                else:
                    logger.warning(f"âŒ [TOKEN BUCKET] Insufficient tokens for API key {api_key_id}, available: {bucket.tokens:.2f}, required: {tokens}")
                    return False
                
        except Exception as e:
            logger.error(f"ğŸ’¥ [TOKEN BUCKET] Error consuming token for API key {api_key_id}: {e}")
            return False
    
    def get_available_tokens(self, api_key_id: int) -> float:
        """è·å–æŒ‡å®š API key çš„å¯ç”¨ä»¤ç‰Œæ•°"""
        try:
            bucket = self._get_bucket(api_key_id)
            bucket = self._refill_bucket(bucket)
            self._save_bucket(api_key_id, bucket)
            return bucket.tokens
        except Exception as e:
            logger.error(f"Error getting available tokens for API key {api_key_id}: {e}")
            return 0.0
    
    def get_bucket_info(self, api_key_id: int) -> Dict[str, Any]:
        """è·å–ä»¤ç‰Œæ¡¶çš„è¯¦ç»†ä¿¡æ¯"""
        try:
            bucket = self._get_bucket(api_key_id)
            bucket = self._refill_bucket(bucket)
            self._save_bucket(api_key_id, bucket)
            return bucket.to_dict()
        except Exception as e:
            logger.error(f"Error getting bucket info for API key {api_key_id}: {e}")
            return {}
    
    def reset_bucket(self, api_key_id: int):
        """é‡ç½®ä»¤ç‰Œæ¡¶ï¼ˆå¡«æ»¡ä»¤ç‰Œï¼‰"""
        try:
            bucket = self._get_bucket(api_key_id)
            bucket.tokens = bucket.capacity
            bucket.last_refill = time.time()
            self._save_bucket(api_key_id, bucket)
            logger.info(f"Reset token bucket for API key {api_key_id}")
        except Exception as e:
            logger.error(f"Error resetting bucket for API key {api_key_id}: {e}")
    
    def configure_bucket(self, api_key_id: int, capacity: int = None, refill_rate: float = None):
        """é…ç½®ä»¤ç‰Œæ¡¶å‚æ•°"""
        try:
            bucket = self._get_bucket(api_key_id)
            
            if capacity is not None:
                bucket.capacity = capacity
                # å¦‚æœå½“å‰ä»¤ç‰Œæ•°è¶…è¿‡æ–°å®¹é‡ï¼Œåˆ™è°ƒæ•´
                bucket.tokens = min(bucket.tokens, capacity)
            
            if refill_rate is not None:
                bucket.refill_rate = refill_rate
            
            self._save_bucket(api_key_id, bucket)
            logger.info(f"Configured bucket for API key {api_key_id}: capacity={bucket.capacity}, refill_rate={bucket.refill_rate}")
            
        except Exception as e:
            logger.error(f"Error configuring bucket for API key {api_key_id}: {e}")
    
    def get_available_api_keys(self, api_key_ids: List[int], required_tokens: int = 1) -> List[int]:
        """
        è·å–æœ‰è¶³å¤Ÿä»¤ç‰Œçš„ API key åˆ—è¡¨ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼Œæ‰¹é‡æ£€æŸ¥ï¼‰
        
        Args:
            api_key_ids: API key ID åˆ—è¡¨
            required_tokens: æ‰€éœ€ä»¤ç‰Œæ•°
            
        Returns:
            List[int]: æœ‰è¶³å¤Ÿä»¤ç‰Œçš„ API key ID åˆ—è¡¨
        """
        if not api_key_ids:
            return []
        
        logger.info(f"ğŸ” [TOKEN BUCKET] Batch checking {len(api_key_ids)} API keys for {required_tokens} tokens")
        
        # ä½¿ç”¨æ‰¹é‡è·å–æ–¹æ³•
        tokens_map = self.get_available_tokens_batch(api_key_ids)
        
        available_keys = []
        for api_key_id in api_key_ids:
            tokens = tokens_map.get(api_key_id, 0.0)
            if tokens >= required_tokens:
                available_keys.append(api_key_id)
                logger.debug(f"âœ… [TOKEN BUCKET] API key {api_key_id} has {tokens:.2f} tokens (sufficient)")
            else:
                logger.debug(f"âŒ [TOKEN BUCKET] API key {api_key_id} has {tokens:.2f} tokens (insufficient)")
        
        logger.info(f"ğŸ“Š [TOKEN BUCKET] Batch check result: {len(available_keys)}/{len(api_key_ids)} keys have sufficient tokens")
        return available_keys
    
    def get_available_tokens_batch(self, api_key_ids: List[int]) -> Dict[int, float]:
        """
        æ‰¹é‡è·å–å¤šä¸ª API key çš„å¯ç”¨ä»¤ç‰Œæ•°ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼Œä½¿ç”¨pipelineï¼‰
        
        Args:
            api_key_ids: API key ID åˆ—è¡¨
            
        Returns:
            Dict[int, float]: API key ID åˆ°å¯ç”¨ä»¤ç‰Œæ•°çš„æ˜ å°„
        """
        if not api_key_ids:
            return {}
        
        logger.debug(f"ğŸ” [TOKEN BUCKET] Batch getting tokens for {len(api_key_ids)} API keys")
        
        tokens_map = {}
        current_time = time.time()
        
        # æ‰¹é‡è·å–æ‰€æœ‰æ¡¶çš„æ•°æ®
        bucket_keys = [self._get_bucket_key(api_key_id) for api_key_id in api_key_ids]
        
        try:
            # ä½¿ç”¨pipelineæ‰¹é‡è·å–æ•°æ®
            pipe = self.redis_client.pipeline()
            for key in bucket_keys:
                pipe.get(key)
            bucket_data_list = pipe.execute()
            
            # å¤„ç†æ¯ä¸ªæ¡¶çš„ä»¤ç‰Œæ•°
            for api_key_id, bucket_data in zip(api_key_ids, bucket_data_list):
                try:
                    if bucket_data:
                        data = json.loads(bucket_data)
                        bucket = TokenBucket.from_dict(data)
                        
                        # è¡¥å……ä»¤ç‰Œ
                        time_passed = current_time - bucket.last_refill
                        if time_passed > 0:
                            tokens_to_add = time_passed * bucket.refill_rate
                            bucket.tokens = min(bucket.capacity, bucket.tokens + tokens_to_add)
                        
                        tokens_map[api_key_id] = bucket.tokens
                        logger.debug(f"âœ… [TOKEN BUCKET] API key {api_key_id} has {bucket.tokens:.2f} tokens")
                    else:
                        # æ–°æ¡¶é»˜è®¤æ˜¯æ»¡çš„
                        tokens_map[api_key_id] = self.default_capacity
                        logger.debug(f"ğŸ†• [TOKEN BUCKET] New bucket for API key {api_key_id} with {self.default_capacity} tokens")
                        
                except Exception as e:
                    logger.warning(f"âš ï¸ [TOKEN BUCKET] Error getting tokens for API key {api_key_id}: {e}")
                    tokens_map[api_key_id] = 0.0
            
            logger.debug(f"ğŸ“Š [TOKEN BUCKET] Batch token retrieval completed for {len(tokens_map)} keys")
            return tokens_map
            
        except Exception as e:
            logger.error(f"ğŸ’¥ [TOKEN BUCKET] Error in batch token retrieval, falling back to individual checks: {e}")
            # å›é€€åˆ°é€ä¸ªæ£€æŸ¥
            for api_key_id in api_key_ids:
                try:
                    tokens_map[api_key_id] = self.get_available_tokens(api_key_id)
                except Exception as individual_error:
                    logger.error(f"Error getting tokens for API key {api_key_id}: {individual_error}")
                    tokens_map[api_key_id] = 0.0
            
            return tokens_map
    
    def cleanup_expired_buckets(self):
        """æ¸…ç†è¿‡æœŸçš„ä»¤ç‰Œæ¡¶ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼Œä½¿ç”¨SCANé¿å…é˜»å¡ï¼‰"""
        try:
            pattern = f"{self.bucket_prefix}*"
            current_time = time.time()
            total_deleted = 0
            cursor = 0
            
            while True:
                # ä½¿ç”¨SCANè€Œä¸æ˜¯KEYSï¼Œé¿å…é˜»å¡Redis
                cursor, keys = self.redis_client.scan(cursor, match=pattern, count=100)
                
                if keys:
                    expired_keys = []
                    
                    # æ‰¹é‡è·å–æ•°æ®
                    pipe = self.redis_client.pipeline()
                    for key in keys:
                        pipe.get(key)
                    bucket_data_list = pipe.execute()
                    
                    # æ£€æŸ¥è¿‡æœŸ
                    for key, bucket_data in zip(keys, bucket_data_list):
                        if bucket_data:
                            try:
                                data = json.loads(bucket_data)
                                bucket = TokenBucket.from_dict(data)
                                # å¦‚æœè¶…è¿‡1å°æ—¶æ²¡æœ‰ä½¿ç”¨ï¼Œè®¤ä¸ºè¿‡æœŸ
                                if current_time - bucket.last_refill > 3600:
                                    expired_keys.append(key)
                            except (json.JSONDecodeError, TypeError):
                                expired_keys.append(key)
                        else:
                            # æ•°æ®å·²ç»ä¸å­˜åœ¨ï¼Œå¯èƒ½æ˜¯TTLè¿‡æœŸäº†
                            expired_keys.append(key)
                    
                    # æ‰¹é‡åˆ é™¤è¿‡æœŸçš„é”®
                    if expired_keys:
                        self.redis_client.delete(*expired_keys)
                        total_deleted += len(expired_keys)
                        logger.debug(f"Cleaned up {len(expired_keys)} expired buckets in this batch")
                
                if cursor == 0:
                    break
            
            if total_deleted > 0:
                logger.info(f"Cleaned up {total_deleted} expired token buckets")
            
            return total_deleted
                
        except Exception as e:
            logger.error(f"Error cleaning up expired buckets: {e}")
            return 0


class OptimizedTokenBucketManager(TokenBucketManager):
    """ä¼˜åŒ–ç‰ˆä»¤ç‰Œæ¡¶ç®¡ç†å™¨ï¼Œç»§æ‰¿åŸæœ‰ç±»å¹¶å¢å¼ºåŠŸèƒ½"""
    
    def __init__(self, redis_client: Optional[redis.Redis] = None):
        super().__init__(redis_client)
        
        # å†…å­˜ç¼“å­˜ï¼Œç”¨äºå‡å°‘RedisæŸ¥è¯¢
        self._token_cache = {}  # {api_key_id: (tokens, timestamp)}
        self._cache_ttl = 5  # ç¼“å­˜5ç§’
        
        # å°è¯•æ³¨å†ŒLuaè„šæœ¬
        try:
            self._lua_script = self.redis_client.register_script(self._consume_token_lua_script())
            logger.info("ğŸš€ [TOKEN BUCKET] Optimized token bucket manager initialized with Lua script support and caching")
        except Exception as e:
            logger.warning(f"âš ï¸ [TOKEN BUCKET] Failed to register Lua script, falling back to original implementation: {e}")
            self._lua_script = None
    
    def _is_cache_valid(self, api_key_id: int) -> bool:
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ"""
        if api_key_id not in self._token_cache:
            return False
        
        _, cached_time = self._token_cache[api_key_id]
        return time.time() - cached_time < self._cache_ttl
    
    def _get_cached_tokens(self, api_key_id: int) -> Optional[float]:
        """ä»ç¼“å­˜è·å–ä»¤ç‰Œæ•°"""
        if self._is_cache_valid(api_key_id):
            tokens, _ = self._token_cache[api_key_id]
            return tokens
        return None
    
    def _cache_tokens(self, api_key_id: int, tokens: float):
        """ç¼“å­˜ä»¤ç‰Œæ•°"""
        self._token_cache[api_key_id] = (tokens, time.time())
    
    def _clear_expired_cache(self):
        """æ¸…ç†è¿‡æœŸçš„ç¼“å­˜é¡¹"""
        current_time = time.time()
        expired_keys = [
            key for key, (_, cached_time) in self._token_cache.items()
            if current_time - cached_time >= self._cache_ttl
        ]
        for key in expired_keys:
            del self._token_cache[key]
    
    def get_available_tokens_batch(self, api_key_ids: List[int]) -> Dict[int, float]:
        """
        æ‰¹é‡è·å–å¤šä¸ª API key çš„å¯ç”¨ä»¤ç‰Œæ•°ï¼ˆå¸¦ç¼“å­˜ä¼˜åŒ–ï¼‰
        """
        if not api_key_ids:
            return {}
        
        # æ¸…ç†è¿‡æœŸç¼“å­˜
        self._clear_expired_cache()
        
        # æ£€æŸ¥å“ªäº›keyéœ€è¦ä»Redisè·å–
        tokens_map = {}
        keys_to_fetch = []
        
        for api_key_id in api_key_ids:
            cached_tokens = self._get_cached_tokens(api_key_id)
            if cached_tokens is not None:
                tokens_map[api_key_id] = cached_tokens
                logger.debug(f"ğŸ¯ [TOKEN BUCKET] Using cached tokens for API key {api_key_id}: {cached_tokens:.2f}")
            else:
                keys_to_fetch.append(api_key_id)
        
        # æ‰¹é‡è·å–æœªç¼“å­˜çš„key
        if keys_to_fetch:
            logger.debug(f"ğŸ” [TOKEN BUCKET] Fetching tokens from Redis for {len(keys_to_fetch)} keys")
            fresh_tokens = super().get_available_tokens_batch(keys_to_fetch)
            
            # æ›´æ–°ç¼“å­˜å’Œç»“æœ
            for api_key_id, tokens in fresh_tokens.items():
                self._cache_tokens(api_key_id, tokens)
                tokens_map[api_key_id] = tokens
        
        logger.debug(f"ğŸ“Š [TOKEN BUCKET] Batch token retrieval completed: {len(tokens_map)} keys ({len(api_key_ids) - len(keys_to_fetch)} from cache, {len(keys_to_fetch)} from Redis)")
        return tokens_map
    
    def get_bucket_status(self, api_key_id: int) -> Dict[str, Any]:
        """è·å–ä»¤ç‰Œæ¡¶çŠ¶æ€ä¿¡æ¯ï¼ˆç”¨äºç›‘æ§ï¼‰"""
        try:
            bucket_info = self.get_bucket_info(api_key_id)
            if bucket_info:
                capacity = bucket_info.get("capacity", 0)
                tokens = bucket_info.get("tokens", 0)
                utilization = round((1 - tokens / max(capacity, 1)) * 100, 2) if capacity > 0 else 0
                
                status = {
                    "api_key_id": api_key_id,
                    "capacity": capacity,
                    "tokens": round(tokens, 2),
                    "refill_rate": bucket_info.get("refill_rate", 0),
                    "utilization_percent": utilization,
                    "status": "healthy" if tokens > 0 else "depleted",
                    "last_refill": bucket_info.get("last_refill", 0)
                }
                
                logger.debug(f"ğŸ“Š [TOKEN BUCKET] Status for API key {api_key_id}: {status['status']}, {tokens:.2f}/{capacity} tokens")
                return status
            else:
                return {
                    "api_key_id": api_key_id,
                    "status": "not_found"
                }
        except Exception as e:
            logger.error(f"ğŸ’¥ [TOKEN BUCKET] Error getting bucket status for API key {api_key_id}: {e}")
            return {"api_key_id": api_key_id, "status": "error", "error": str(e)}


# å…¨å±€ä»¤ç‰Œæ¡¶ç®¡ç†å™¨å®ä¾‹ - ä½¿ç”¨ä¼˜åŒ–ç‰ˆæœ¬
token_bucket_manager = OptimizedTokenBucketManager()